My project for this course was an AI customer service agent trained on sample e-commerce data from an online clothing store. The goal was to gain initial experience working with AI models and to better understand how they operate. This project was built using Karpathy’s GitHub repository, nanoGPT.
The primary aim was to gain practical experience with AI models and learn how to modify them to meet specific needs. This project is significant to me because AI is becoming increasingly central to computer science fields, and I believe future roles will revolve heavily around it. I also find the potential problems AI can solve both fascinating and highly applicable to real-world scenarios.
Much of my work relied on Karpathy’s nanoGPT repository. I forked the repository, integrated my own dataset, and found his YouTube tutorial helpful for understanding the code and model operations. For my data, I used Kaggle, a platform that hosts extensive datasets for AI and machine learning projects.
Some new Python libraries I explored included OpenAI, tiktoken, and Pandas. Using these, I created a custom data-cleaning script to format my dataset and make it usable for training. While Karpathy’s tutorial uses character-level tokens, I opted for tiktoken’s GPT-2 encoder, which processes larger chunks of text as tokens.
Because I did not use a virtual machine, I was limited to training on my CPU. My hardware lacked CUDA-enabled GPUs, which restricted the model’s training speed and efficiency. As a result, much of my effort went into data preparation and cleaning rather than extended training.
From this project, I learned the critical importance of high-quality training data. A model cannot produce strong outputs without good inputs. Initially, I thought these models could “figure out” the good and bad data automatically, but I now understand that generative models, especially those trained on open web content, can produce flawed outputs if the training data is poor. I learned how to extract data using API tokens, clean it, and build efficient pipelines to keep models updated with relevant, accurate information.
When I sampled my model’s outputs, I noticed they reflected my training data: robotic, grammatically incorrect, and often nonsensical. This made sense in hindsight and reinforced the importance of carefully curated datasets.
My original plan included setting up verification tests to assess chatbot accuracy and visualizing loss through plots. I did not achieve these goals within the project’s timeframe. To meet them, I would either need to use a virtual machine with a pre-trained model for fine-tuning or build an encoder to query a smaller model compatible with nanoGPT. This would allow me to track loss data during training and visualize it using tools like Pandas.
Despite these limitations, I achieved some of my long-term objectives, such as gaining a baseline understanding of AI models and the workflow involved in using them. I still have much to learn, particularly regarding model selection, data optimization, and hardware requirements. I also realized I need to plan projects more thoroughly, considering algorithms, libraries, and infrastructure in advance.
I’m proud that I was able to generate sample outputs from my model, but I want to deepen my understanding of the underlying code to improve results. Initially, I wasn’t sure if this area of computer science would interest me, but as I progressed, I found it to be an exciting blend of software engineering and data science.
Looking ahead, I plan to fully complete this project. My next steps include refining my repository, building an encoder architecture, and potentially creating a sample website linked to a database. This would allow automated updates to my training data and a more robust pipeline. Ultimately, I want to demonstrate the ability to manage an AI project end-to-end, from data preparation to deployment and I plan to continue working toward that goal over the next few semesters.


References or Bibliography:
https://github.com/karpathy/nanoGPT
https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
https://www.youtube.com/watch?v=kCc8FmEb1nY
