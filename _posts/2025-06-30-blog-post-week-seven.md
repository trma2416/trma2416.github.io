What I did last week was explore environments that I can run my code on efficiently. I found that my computer will not run my code on my GPU given my old computer and that training requires a long amount of time and computation to train my model. I also tweaked some of my training parameters such as learning rate and head size and embedded dimensions and dropout rate to see how my computer compared which I found is not as efficient as what was provided by Karpathy’s codebases. 

This week I plan on working on the pretraining of my model and tweaking my parameters based on various literature and architecture to ensure that my model is training as efficiently as possible. I need to work on running my model on virtual GPU’s that make it more efficient and modifying my model such that it best suits my needs. Karpahty’s codebase is focussed on pre-training models and not creating a fully functioning input-output model so I need to focus on the most efficient way to train my model and get it producing proficient output data and minimizing the loss values. 

Now that there are clear goals for what I can accomplish given the time being I feel there are not many impediments in my way of achieving these goals besides repetitive tasks such as uploading to github and documenting my code. 

This week I plan on being more efficient in reading literature about the best practices in refining my model and taking less time coding and more time understanding. I feel that this is a better use of my time this week and I think it will help me in the long run of my project, so I guess the goal is slowing down and focusing on the fundamentals and statistics. 
