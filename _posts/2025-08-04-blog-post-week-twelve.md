Last week, I successfully got my data-cleaning scripts working and moved them from the test .ipynb notebooks into .py files. I was able to get them functioning by modifying my prompting, similar to what Professor Guinn suggested. I’ve also been working on adjusting the training and model scripts so I can begin training on my dataset.

This week, my plan is to finalize my scripts and start the training process. I aim to produce some sort of output by the end of the week, then scale up computational resources to create a more “usable” system. The main repository offers good suggestions and alternatives for running smaller datasets locally by adjusting certain parameters. Since my dataset contains around 20,000 tokens, I think this approach is feasible.

For the rest of the week, I don’t anticipate major impediments. My main focus will be debugging and getting my code fully operational.

My process last week was effective, and I believe that if I continue with the same approach, I should be able to produce a reasonable result by the end of the course.